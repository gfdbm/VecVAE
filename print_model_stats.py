#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""æ‰“å° VP-VAE æ¨¡å‹çš„å‚æ•°ç»Ÿè®¡"""

import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

try:
    import torch
    from src.model.encoder import VpVaeEncoder, VpVaeEncoderConfig
    from src.model.decoder import Decoder, DecoderConfig
    
    def count_parameters(model):
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
        total = sum(p.numel() for p in model.parameters())
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        return total, trainable
    
    def format_number(num):
        """æ ¼å¼åŒ–æ•°å­—"""
        if num >= 1e6:
            return f"{num/1e6:.2f}M"
        elif num >= 1e3:
            return f"{num/1e3:.2f}K"
        else:
            return str(num)
    
    print("\n" + "="*80)
    print("ğŸ” VP-VAE æ¨¡å‹å‚æ•°ç»Ÿè®¡")
    print("="*80)
    
    # é»˜è®¤é…ç½®
    print("\nğŸ“ é…ç½®å‚æ•°:")
    print("-"*80)
    embed_dim = 256
    num_heads = 8
    z_dim = 128
    cross_layers = 1
    dec_layers = 4
    patch_size = 16
    
    print(f"  åµŒå…¥ç»´åº¦ (embed_dim):     {embed_dim}")
    print(f"  æ³¨æ„åŠ›å¤´æ•° (num_heads):   {num_heads}")
    print(f"  æ½œåœ¨ç»´åº¦ (z_dim):         {z_dim}")
    print(f"  è·¨æ³¨æ„åŠ›å±‚æ•° (enc-xlayers): {cross_layers}")
    print(f"  è§£ç å™¨å±‚æ•° (dec-layers):   {dec_layers}")
    print(f"  Patch å¤§å° (patch_size):  {patch_size}")
    
    # åˆ›å»ºç¼–ç å™¨
    print("\n" + "="*80)
    print("ğŸ§  ç¼–ç å™¨ (Encoder)")
    print("="*80)
    
    enc_config = VpVaeEncoderConfig(
        embed_dim=embed_dim,
        num_heads=num_heads,
        cross_layers=cross_layers,
        patch_size=patch_size,
        z_dim=z_dim,
        use_prefix_repr=True,
        dropout=0.0
    )
    encoder = VpVaeEncoder(enc_config)
    enc_total, enc_trainable = count_parameters(encoder)
    
    print(f"\n1. VectorPrefixEncoder (çŸ¢é‡ç¼–ç )")
    print(f"   - 3 å±‚ Transformer (å›ºå®š)")
    print(f"   - {num_heads} ä¸ªæ³¨æ„åŠ›å¤´")
    print(f"   - FFN ä¸­é—´å±‚: {embed_dim} â†’ {embed_dim*4} â†’ {embed_dim}")
    vec_params, _ = count_parameters(encoder.vec)
    print(f"   å‚æ•°é‡: {format_number(vec_params)}")
    
    print(f"\n2. PixelEncoder (åƒç´ ç¼–ç )")
    print(f"   - CNN + Patch Embedding")
    print(f"   - Patch å¤§å°: {patch_size}Ã—{patch_size}")
    pix_params, _ = count_parameters(encoder.pix)
    print(f"   å‚æ•°é‡: {format_number(pix_params)}")
    
    print(f"\n3. CrossAttentionAdapter (è·¨æ¨¡æ€èåˆ)")
    print(f"   - {cross_layers} å±‚è·¨æ³¨æ„åŠ›")
    print(f"   - {num_heads} ä¸ªæ³¨æ„åŠ›å¤´")
    xattn_params, _ = count_parameters(encoder.xattn)
    print(f"   å‚æ•°é‡: {format_number(xattn_params)}")
    
    print(f"\n4. PosteriorHead (æ½œåœ¨ç©ºé—´æŠ•å½±)")
    print(f"   - è¾“å…¥ç»´åº¦: {embed_dim}")
    print(f"   - æ½œåœ¨ç»´åº¦: {z_dim}")
    head_params, _ = count_parameters(encoder.head)
    print(f"   å‚æ•°é‡: {format_number(head_params)}")
    
    print(f"\n{'ç¼–ç å™¨æ€»å‚æ•°é‡:':<30} {format_number(enc_total):<15} ({enc_total:,})")
    
    # åˆ›å»ºè§£ç å™¨
    print("\n" + "="*80)
    print("ğŸ¨ è§£ç å™¨ (Decoder)")
    print("="*80)
    
    dec_config = DecoderConfig(
        vocab_size=9,
        max_len=256,
        embed_dim=embed_dim,
        z_dim=z_dim,
        n_heads=num_heads,
        n_layers=dec_layers,
        patch_size=patch_size,
        use_pixel_cross_attn=True
    )
    decoder = Decoder(dec_config)
    dec_total, dec_trainable = count_parameters(decoder)
    
    print(f"\n1. ä½ç½® & æœ‰æ•ˆä½åµŒå…¥")
    print(f"   - ç»å¯¹ä½ç½®: 256 ä¸ªä½ç½®")
    print(f"   - æœ‰æ•ˆä½: 2 ç±» (å‰ç¼€/å¡«å……)")
    print(f"   å‚æ•°é‡: ~0.1M")
    
    if hasattr(decoder, 'pix'):
        print(f"\n2. PixelEncoder (åƒç´ ç¼–ç )")
        print(f"   - ä¸ç¼–ç å™¨å…±äº«ç»“æ„")
        dec_pix_params, _ = count_parameters(decoder.pix)
        print(f"   å‚æ•°é‡: {format_number(dec_pix_params)}")
    
    print(f"\n3. Transformer è§£ç å— (Ã—{dec_layers} å±‚)")
    print(f"   æ¯å±‚ç»“æ„:")
    print(f"   â”œâ”€ Self-Attention ({num_heads} å¤´)")
    print(f"   â”œâ”€ Cross-Attention (ä¸åƒç´ )")
    print(f"   â””â”€ FFN ({embed_dim} â†’ {embed_dim*4} â†’ {embed_dim})")
    
    # ä¼°ç®—æ¯å±‚å‚æ•°
    single_layer_params = dec_total / dec_layers * 0.7  # ç²—ç•¥ä¼°ç®—
    print(f"   æ¯å±‚çº¦: {format_number(single_layer_params)}")
    
    print(f"\n4. è¾“å‡ºå¤´")
    print(f"   â”œâ”€ å‘½ä»¤åˆ†ç±»å¤´: â†’ 9 ç±»")
    print(f"   â””â”€ åæ ‡å›å½’å¤´: â†’ 4 ç»´")
    out_params, _ = count_parameters(decoder.head_cmd)
    out_params += sum(p.numel() for p in decoder.head_arg.parameters())
    print(f"   å‚æ•°é‡: {format_number(out_params)}")
    
    print(f"\n{'è§£ç å™¨æ€»å‚æ•°é‡:':<30} {format_number(dec_total):<15} ({dec_total:,})")
    
    # æ€»è®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»å‚æ•°ç»Ÿè®¡")
    print("="*80)
    total_params = enc_total + dec_total
    print(f"\n{'ç¼–ç å™¨:':<20} {format_number(enc_total):>15} ({enc_total:>12,})")
    print(f"{'è§£ç å™¨:':<20} {format_number(dec_total):>15} ({dec_total:>12,})")
    print(f"{'-'*20} {'-'*15} {'-'*15}")
    print(f"{'VP-VAE æ€»è®¡:':<20} {format_number(total_params):>15} ({total_params:>12,})")
    
    # ä¼°ç®—æ˜¾å­˜ (FP32)
    memory_mb = total_params * 4 / (1024**2)  # 4 bytes per float32
    print(f"\nğŸ’¾ æ¨¡å‹å¤§å°ä¼°ç®— (FP32):")
    print(f"   å‚æ•°: ~{memory_mb:.1f} MB")
    print(f"   è®­ç»ƒæ—¶ (å«æ¢¯åº¦+ä¼˜åŒ–å™¨): ~{memory_mb*3:.1f} MB")
    print(f"   å®é™…æ˜¾å­˜éœ€æ±‚ (batch=8): ~{memory_mb*3 + 2000:.1f} MB (~{(memory_mb*3 + 2000)/1024:.1f} GB)")
    
    # å¯¹æ¯”ä¸åŒé…ç½®
    print("\n" + "="*80)
    print("âš™ï¸  ä¸åŒé…ç½®çš„å‚æ•°é‡å¯¹æ¯”")
    print("="*80)
    
    configs = [
        ("è½»é‡", 128, 64, 1, 2),
        ("æ ‡å‡† (å½“å‰)", 256, 128, 1, 4),
        ("å¢å¼º", 256, 256, 2, 6),
        ("é‡å‹", 512, 256, 2, 8),
    ]
    
    print(f"\n{'é…ç½®':<15} {'embed':<8} {'zdim':<8} {'enc-x':<8} {'dec-L':<8} {'å‚æ•°é‡':<15} {'æ˜¾å­˜ä¼°ç®—'}")
    print("-"*80)
    
    for name, emb, zd, encx, decl in configs:
        # ç²—ç•¥ä¼°ç®—
        enc_est = 2.5e6 * (emb/256)**2 + 0.5e6 * encx
        dec_est = 1e6 * decl * (emb/256)**2
        total_est = enc_est + dec_est
        mem_est = total_est * 4 / (1024**2) * 3 + 2000
        marker = " â† å½“å‰" if name.endswith("å½“å‰)") else ""
        print(f"{name:<15} {emb:<8} {zd:<8} {encx:<8} {decl:<8} {format_number(total_est):<15} ~{mem_est/1024:.1f}GB{marker}")
    
    print("\n" + "="*80)
    print("ğŸ’¡ è°ƒå‚å»ºè®®")
    print("="*80)
    print("""
1. å¦‚æœæ˜¾å­˜ä¸è¶³:
   python train.py --embed 128 --dec-layers 2 --batch 4

2. å¦‚æœ L1 æŸå¤±ä¸ä¸‹é™:
   python train.py --dec-layers 6 --zdim 256

3. å¦‚æœæƒ³è¦æ›´å¥½æ•ˆæœ (æ˜¾å­˜å……è¶³):
   python train.py --embed 512 --dec-layers 6 --batch 4

4. å¿«é€ŸåŸå‹ (åŠ å¿«è®­ç»ƒ):
   python train.py --embed 128 --dec-layers 2 --enc-xlayers 1
    """)
    print("="*80 + "\n")
    
except ImportError as e:
    print(f"\nâŒ å¯¼å…¥å¤±è´¥: {e}")
    print("   è¯·ç¡®ä¿å·²å®‰è£… PyTorch å¹¶ä¸”åœ¨æ­£ç¡®çš„é¡¹ç›®ç›®å½•ä¸‹è¿è¡Œ")
    print()
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()

